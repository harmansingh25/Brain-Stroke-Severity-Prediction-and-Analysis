# -*- coding: utf-8 -*-
"""Stroke.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-ov0QVF7CnHltRfgUqB_WqiqcNEcVEi6

#Importing modules
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from math import sqrt
from google.colab import drive
import sklearn
from sklearn.linear_model import LogisticRegressionCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import MinMaxScaler
from imblearn.combine import SMOTETomek
import seaborn as sns
from collections import Counter
import warnings
from numpy import mean
from numpy import std
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import validation_curve
from numpy import arange
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import log_loss
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import AdaBoostClassifier
from sklearn.neural_network import MLPClassifier

"""#Reading the data"""

warnings.filterwarnings('ignore')
url = "https://raw.githubusercontent.com/harmansingh25/ML_Project_2021/main/Stroke_analysis1%20-%20Stroke_analysis1.csv"
df = pd.read_csv(url)
df

"""# Dropping unnecessary columns"""

df = df.drop(["Unnamed: 0"], axis = 1)
df = df.drop(["pid"], axis = 1)
df = df.rename(columns={"nhiss":"NIHSS_Score", "mrs":"mRS", "distolic":"diastolic"})

"""# EDA"""

df

#Printing the shape of the dataset
df.shape

#Describing the dataset
df.describe()

#Getting information about the dataset
df.info()

#Getting the ditribution of the target variable risk
df.risk.value_counts().plot(kind='pie',autopct='%1.1f%%')

#Getting distribution on the basis of age
fig, ax=plt.subplots(figsize=(20,5))
sns.countplot(x=df.age, palette='viridis')
plt.xticks(rotation=90)
plt.xlabel('\n Age', fontsize=10, fontweight='bold')
plt.ylabel('Count of Patients', fontsize=10, fontweight='bold')
plt.title('Age of Different Patients', fontweight = 'bold', fontsize='15')
plt.show()

#Getting the ages where stroke severity is higher
stroke_0 = df[~(df['risk'] == 0)]
stroke_1 = df[~(df['risk'] == 1)]
stroke_2 = df[~(df['risk'] == 2)]
stroke_3 = df[~(df['risk'] == 3)]
sns.set(style="darkgrid")
fig3, (ax1, ax2) = plt.subplots(2,1, figsize=(15, 9))
fig4, (ax3, ax4) = plt.subplots(2,1, figsize=(15, 9))

sns.histplot(x=stroke_0['age'], kde=True, color="skyblue", ax=ax1)
sns.histplot(x=stroke_1['age'], kde=True, color="olive", ax=ax2)
sns.histplot(x=stroke_2['age'], kde=True, color="darkorchid", ax=ax3)
sns.histplot(x=stroke_3['age'], kde=True, color="coral", ax=ax4)

#Getting the severity of strokes age-wise
fig4, ax=plt.subplots(figsize=(20,5))
legend = ['No Stroke', 'Stroke']
sns.set(style="darkgrid")
sns.histplot(x=df['age'], hue=df.risk, palette='rocket')
plt.xlabel('\n Age', fontsize=10, fontweight='bold')
plt.ylabel('Count of Patients', fontsize=10, fontweight='bold')
plt.title('Age of Different Patients', fontweight = 'bold', fontsize='15')
plt.show()

#Getting average glucose levels across genders
fig11, ax = plt.subplots(figsize=(5, 5))
sns.set(style="darkgrid")

sns.barplot(x=df.gender,y=df.glucose,hue=df.risk,estimator=np.average ,ci=None,palette='icefire')
plt.xlabel('Gender & Stroke',fontsize=10, fontweight='bold')
plt.ylabel('Average Glusoce Level',fontsize=10, fontweight='bold')
plt.title('Average Glusoce Level v/s Gender',fontsize=15, fontweight='bold')


plt.show()

#Getting average BMI levels across genders
fig11, ax = plt.subplots(figsize=(5, 5))
sns.set(style="darkgrid")

sns.barplot(x=df.gender,y=df.bmi,hue=df.risk,estimator=np.average ,ci=None,palette='icefire')
plt.xlabel('BMI & Stroke',fontsize=10, fontweight='bold')
plt.ylabel('Average BLI Level',fontsize=10, fontweight='bold')
plt.title('Average BMI Level v/s Gender',fontsize=15, fontweight='bold')


plt.show()

#Getting average Cholestrol levels across genders
fig11, ax = plt.subplots(figsize=(5, 5))
sns.set(style="darkgrid")

sns.barplot(x=df.gender,y=df.cholestrol,hue=df.risk,estimator=np.average ,ci=None,palette='icefire')
plt.xlabel('Cholestrol & Stroke',fontsize=10, fontweight='bold')
plt.ylabel('Average Cholestrol Level',fontsize=10, fontweight='bold')
plt.title('Average Cholestrol level v/s Gender',fontsize=15, fontweight='bold')


plt.show()

#Getting average Systolic blood pressure levels across genders
fig11, ax = plt.subplots(figsize=(5, 5))
sns.set(style="darkgrid")

sns.barplot(x=df.gender,y=df.systolic,hue=df.risk,estimator=np.average ,ci=None,palette='icefire')
plt.xlabel('Systolic BP & Stroke',fontsize=10, fontweight='bold')
plt.ylabel('Average Systolic BP Level',fontsize=10, fontweight='bold')
plt.title('Average Systolic BP level v/s Gender',fontsize=15, fontweight='bold')

plt.show()

#Getting average Diastolic blood pressure levels across genders
fig11, ax = plt.subplots(figsize=(5, 5))
sns.set(style="darkgrid")

sns.barplot(x=df.gender,y=df.diastolic,hue=df.risk,estimator=np.average ,ci=None,palette='icefire')
plt.xlabel('Diastolic BP & Stroke',fontsize=10, fontweight='bold')
plt.ylabel('Average Diastolic BP Level',fontsize=10, fontweight='bold')
plt.title('Average Diastolic BP level v/s Gender',fontsize=15, fontweight='bold')

plt.show()

#Smoking habits of male and females vs stroke severity
fig13, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
fig14, (ax3, ax4) = plt.subplots(1, 2, figsize=(15, 5))

sns.countplot(x=stroke_0.smoking,hue=stroke_0.gender, palette='viridis',ax=ax1)
ax1.set_xlabel('Smoking Habits Stroke severity 0',fontsize=10, fontweight='bold')
ax1.set_yticks([])
ax1.set_ylabel('Patients',fontsize=10, fontweight='bold')
ax1.set_ylabel('Count of patient')
for p in ax1.patches:
    ax1.annotate(f'\n{p.get_height()}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='black', size=10)


sns.countplot(x=stroke_1.smoking,hue=stroke_1.gender, palette='viridis',ax=ax2)
ax2.set_xlabel('Smoking Habits Stroke severity 1',fontsize=10, fontweight='bold')
ax2.set_yticks([])
ax2.set_ylabel('Patients',fontsize=10, fontweight='bold')
ax2.set_ylabel('Count of patient')
for p in ax2.patches:
    ax2.annotate(f'\n{p.get_height()}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='black', size=10)

sns.countplot(x=stroke_2.smoking,hue=stroke_2.gender, palette='viridis',ax=ax3)
ax3.set_xlabel('Smoking Habits Stroke severity 2',fontsize=10, fontweight='bold')
ax3.set_yticks([])
ax3.set_ylabel('Patients',fontsize=10, fontweight='bold')
ax3.set_ylabel('Count of patient')
for p in ax3.patches:
    ax3.annotate(f'\n{p.get_height()}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='black', size=10)


sns.countplot(x=stroke_3.smoking,hue=stroke_3.gender, palette='viridis',ax=ax4)
ax4.set_xlabel('Smoking Habits Stroke severity 3',fontsize=10, fontweight='bold')
ax4.set_yticks([])
ax4.set_ylabel('Patients',fontsize=10, fontweight='bold')
ax4.set_ylabel('Count of patient')
for p in ax4.patches:
    ax4.annotate(f'\n{p.get_height()}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='black', size=10)

#Getting correlation of variables
fig15,ax1 = plt.subplosnsts(figsize=(20,14))
sns.heatmap(df.corr(), annot = True, cmap='magma',ax=ax1)

#Pair plots visualization

g = sns.pairplot(df, hue='risk', markers='+')

plt.show()

"""# Preprocessing

One Hot Encoding 'gender' into Male and Female and dropping 'gender' column
"""

m = {"Male": 1, "Female": 0}
f = {"Male": 0, "Female": 1}
df.insert(1, "Female", df['gender'].map(f), True)
df.insert(1, "Male", df['gender'].map(m), True)
# dropping gender column
df.drop(['gender'], axis=1, inplace=True)
print(df)

zero = df[df['risk']==0]
one = df[df['risk']==1]
two = df[df['risk']==2]
three = df[df['risk']==3]
print("Risk 0:", zero.shape[0])
print("Risk 1:", one.shape[0])
print("Risk 2:", two.shape[0])
print("Risk 3:", three.shape[0])

y = df.values[:, -1].reshape(df.values.shape[0], 1)
X = df.values[:, :-1]

"""# MinMax Scaling"""

scaler = MinMaxScaler()
X = scaler.fit_transform(X)
print(X)

"""##OverSampling"""

smk = SMOTETomek(random_state=42)
X, y = smk.fit_resample(X, y)
print(X)
print("After Oversampling:")
print("Risk", Counter(y))

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

"""#Naive Bayes (Complement)"""

def nb_test(model,x_train,y_train):
  
  print("\n parameters: ")
  #it gives us the default parameters since those are the ones used 
  print( model.get_params(deep=True))
  print("\n log-probability estimates: ")
  print(model.predict_log_proba(x_train))
  print("\n probability estimates: ")
  print(model.predict_proba(x_train))

from sklearn.naive_bayes import ComplementNB
from sklearn.metrics import cohen_kappa_score
#Complement Naive Bayes classifier: particularly suited for imbalanced data sets
#designed to correct the “severe assumptions” made by the standard Multinomial Naive Bayes classifier
#In complement Naive Bayes, instead of calculating the probability of an item belonging to a certain class,
#we calculate the probability of the item belonging to all the classes.
Cmodel = ComplementNB()
Cmodel.fit(x_train,y_train.ravel())
train_pred_c=Cmodel.predict(x_train)
test_pred_c=Cmodel.predict(x_test)
import warnings
warnings.filterwarnings('error')  # ignoring warning or not showing them more than once
warnings.filterwarnings('ignore')
nb_test(Cmodel,x_train,y_train)

print("\n accuracy: ",accuracy_score(y_test, test_pred_c))
cm=confusion_matrix(y_test,test_pred_c)
print("\n Confusion Matrix: ")
print(cm)
sklearn.metrics.plot_confusion_matrix(Cmodel,x_test,y_test,cmap=plt.cm.Blues)
print(classification_report(y_test, test_pred_c))
#print("Cohen Kappa Score",cohen_kappa_score(y_test, test_pred_c))

"""#Logistic Regression

"""

param_grid = {'cv':[3, 5, 8, 10, 12],
              'max_iter': [100, 200, 500, 1000, 2000]}

GS_LR = GridSearchCV(LogisticRegressionCV(), param_grid)
GS_LR.fit(x_train, y_train.ravel())
print(GS_LR.best_params_)
print(GS_LR.score(x_test,y_test))

classifier = LogisticRegressionCV(cv=5, random_state = 0, max_iter=1000)
classifier.fit(x_train, y_train.ravel())
y_pred = classifier.predict(x_test)
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred)*100)
print("Precision:", precision_score(y_test, y_pred, average='macro'))
print("Recall:", recall_score(y_test, y_pred, average='macro'))
print("F1 Score:", f1_score(y_test, y_pred, average='macro'))
print(classification_report(y_test, y_pred))
sklearn.metrics.plot_confusion_matrix(classifier,x_test,y_test,cmap=plt.cm.Blues)

"""##Multilayer Perceptron (MLP)"""

param_grid = {'activation':['relu', 'identity', 'logistic', 'tanh'],
              'max_iter': [50, 100, 150, 200],
              'alpha':[0.1, 0.5, 0.05, 0.01, 0.008],
              'learning_rate_init':[0.1, 0.5, 0.05, 0.01, 0.008]}

GS_MLP = GridSearchCV(MLPClassifier(), param_grid)
GS_MLP.fit(x_train, y_train.ravel())
print(GS_MLP.best_params_)
print(GS_MLP.score(x_test,y_test))

mlp_clf = MLPClassifier(activation='relu', alpha=0.008, learning_rate_init=0.01, max_iter=100)
mlp_clf.fit(x_train, y_train.ravel())
y_pred = mlp_clf.predict(x_test)
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred)*100)
print("Precision:", precision_score(y_test, y_pred, average='macro'))
print("Recall:", recall_score(y_test, y_pred, average='macro'))
print("F1 Score:", f1_score(y_test, y_pred, average='macro'))
print(classification_report(y_test, y_pred))
sklearn.metrics.plot_confusion_matrix(mlp_clf,x_test,y_test,cmap=plt.cm.Blues)

plt.plot(mlp_clf.loss_curve_, label="Train Loss")
mlp_clf.fit(x_test, y_test.ravel())
plt.plot(mlp_clf.loss_curve_, label="Test Loss")
plt.title("MLPClassifier Loss vs Iterations")
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.legend()
plt.show()

activation = ['relu', 'identity', 'logistic', 'tanh']
acti_acc = {}
acti_acc_train = {}
for a in activation:
    mlp_clf = MLPClassifier(activation=a, alpha=0.008, learning_rate_init=0.01, max_iter=100)
    mlp_clf.fit(x_train, y_train.ravel())
    y_pred = mlp_clf.predict(x_test)
    acti_acc[a] = accuracy_score(y_test, y_pred)*100
    y_pred_train = mlp_clf.predict(x_train)
    acti_acc_train[a] = accuracy_score(y_train.ravel(), y_pred_train)*100

plt.title("MLPClassifier Accuracy vs Activation")
plt.xlabel("Activation")
plt.ylabel("Accuracy")
plt.plot(list(acti_acc.keys()), list(acti_acc.values()), label="Test Accuracy")
plt.plot(list(acti_acc_train.keys()), list(acti_acc_train.values()), label="Train Accuracy")
plt.legend()
plt.plot()

learning_rates = [0.001, 0.008, 0.01, 0.05, 0.1]
acti_acc = {}
acti_acc_train = {}
for lr in learning_rates:
    mlp_clf = MLPClassifier(activation='relu', alpha=0.008, learning_rate_init=lr, max_iter=100)
    mlp_clf.fit(x_train, y_train.ravel())
    y_pred = mlp_clf.predict(x_test)
    acti_acc[lr] = accuracy_score(y_test, y_pred)*100
    y_pred_train = mlp_clf.predict(x_train)
    acti_acc_train[lr] = accuracy_score(y_train.ravel(), y_pred_train)*100

plt.title("MLPClassifier Accuracy vs Learning Rate")
plt.xlabel("Learning Rate")
plt.ylabel("Accuracy")
plt.plot(list(acti_acc.keys()), list(acti_acc.values()), label="Test Accuracy")
plt.plot(list(acti_acc_train.keys()), list(acti_acc_train.values()), label="Train Accuracy")
plt.legend()
plt.plot()

"""```
# This is formatted as code
```

# Random Forest

##Understanding the importance of different features
"""

# seeing the feature importance variable
model = RandomForestClassifier(n_estimators = 100)
feature_names=df.columns[:-1]
# Train the model using the training sets
model.fit(x_train, y_train)
feature_imp = pd.Series(model.feature_importances_,index=feature_names ).sort_values(ascending = False)
feature_imp

"""##Training the model

###Finding best parameters using GridSearchCV
"""

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]

param_grid = {'criterion':['gini','entropy'],
              'random_state':[23],
              'bootstrap': [True, False],
              'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, None]}

GS_RF=GridSearchCV(RandomForestClassifier(), param_grid)
GS_RF.fit(x_train,y_train.ravel())
print(GS_RF.best_params_)
print(GS_RF.score(x_test,y_test))

def performance(y_test,y_pred):
  print("accuracy",accuracy_score(y_test, y_pred))
  cm=confusion_matrix(y_test,y_pred)
  print("Confusion matrix")
  print(cm)
  print(classification_report(y_test, y_pred))
  report = classification_report(y_test, y_pred,output_dict=True )
  return report

model=RandomForestClassifier(bootstrap=True, criterion= 'entropy', max_depth= 20, random_state= 23)
model.fit(x_train, y_train.ravel())
yhat = model.predict(x_test)
report=performance(y_test,yhat)
sklearn.metrics.plot_confusion_matrix(model,x_test,y_test,cmap=plt.cm.Blues)
print("score",model.score(x_test,y_test))
print("precision",sklearn.metrics.precision_score(y_test,yhat,average="macro"))
print("recall",sklearn.metrics.recall_score(y_test,yhat,average="macro"))
print("F! Score",sklearn.metrics.f1_score(y_test,yhat,average="macro"))

rf_train_score=[]
rf_test_score=[]
for i in np.arange(1, 35):
  param_grid = {'criterion':['gini','entropy'],'max_depth': [i],'random_state':[23]}
  GS_RF=GridSearchCV(RandomForestClassifier(), param_grid,cv=5)
  GS_RF.fit(x_train,y_train.ravel())
  y_train_pred=GS_RF.predict(x_train)
  rf_train_score.append(accuracy_score(y_train,y_train_pred))
  y_pred=GS_RF.predict(x_test)
  rf_test_score.append(accuracy_score(y_test,y_pred))
 
plt.title("Random Forest Classifier : Accuracy vs Max Depth")
plt.xlabel("Max Depth")
plt.ylabel("Accuracy")
plt.plot(np.arange(1,35),rf_train_score,label="Training Accuracy")
plt.plot(np.arange(1,35),rf_test_score,label="Testing Accuracy")
plt.legend()
plt.plot()

rf_train_score=[]
rf_test_score=[]
n_esti=[50, 100, 200, 400, 600, 800, 1000, 1200]
for i in n_esti:
  param_grid = {'random_state': [23], 'n_estimators': [i], 'min_samples_split': [5],'min_samples_leaf': [2], 'max_depth':[20], 'criterion': ['entropy'], 'bootstrap': ['False']}
  GS_RF=GridSearchCV(RandomForestClassifier(), param_grid,cv=5)
  GS_RF.fit(x_train,y_train.ravel())
  y_train_pred=GS_RF.predict(x_train)
  rf_train_score.append(accuracy_score(y_train,y_train_pred))
  y_pred=GS_RF.predict(x_test)
  rf_test_score.append(accuracy_score(y_test,y_pred))
  # print(rf_train_score,rf_test_score)
 
plt.title("Random Forest Classifier : Accuracy vs Number of Trees")
plt.xlabel("Number of trees")
plt.ylabel("Accuracy")
plt.plot(n_esti,rf_train_score,label="Training Accuracy")
plt.plot(n_esti,rf_test_score,label="Testing Accuracy")
plt.legend()
plt.plot()

"""#Adaboost Classifier"""

param_grid = {'learning_rate': [0.0001,0.001,0.01,0.1,0.3,0.5,0.7,0.9,1],
              'n_estimators': [4,8,10, 20, 30, 40, 50, 60, 70, 80, 90, 100,110,120]}

GS_RF=GridSearchCV(AdaBoostClassifier(), param_grid)
GS_RF.fit(x_train,y_train.ravel())
print(GS_RF.best_params_)
print(GS_RF.score(x_test,y_test))

def performance(y_test,y_pred):
  print("accuracy",accuracy_score(y_test, y_pred))
  cm=confusion_matrix(y_test,y_pred)
  print("Confusion matrix")
  print(cm)
  print(classification_report(y_test, y_pred))
  report = classification_report(y_test, y_pred,output_dict=True )
  return report

model=AdaBoostClassifier(learning_rate=0.9,n_estimators=30)
model.fit(x_train, y_train.ravel())
yhat = model.predict(x_test)
report=performance(y_test,yhat)
sklearn.metrics.plot_confusion_matrix(model,x_test,y_test,cmap=plt.cm.Blues)
print("score",model.score(x_test,y_test))
print("precision",sklearn.metrics.precision_score(y_test,yhat,average="macro"))
print("recall",sklearn.metrics.recall_score(y_test,yhat,average="macro"))
print("F1 Score",sklearn.metrics.f1_score(y_test,yhat,average="macro"))

import pickle
file1 = open("Adaboost", "wb")
pickle.dump(model, file1) #Model trained on grid search parameters
file1.close()

rf_train_score=[]
rf_test_score=[]
for i in np.arange(1, 500,20):
  param_grid = {'n_estimators': [i],'learning_rate':[0.01]}
  GS_RF=GridSearchCV(AdaBoostClassifier(), param_grid)
  GS_RF.fit(x_train,y_train.ravel())
  y_train_pred=GS_RF.predict(x_train)
  rf_train_score.append(accuracy_score(y_train,y_train_pred))
  y_pred=GS_RF.predict(x_test)
  rf_test_score.append(accuracy_score(y_test,y_pred))
  # print(rf_train_score,rf_test_score)
 
plt.title("Ada Boost Classifier : Accuracy vs Number of estimators")
plt.xlabel("Number of estimators")
plt.ylabel("Accuracy")
plt.plot(np.arange(1,500,20),rf_train_score,label="Training Accuracy")
plt.plot(np.arange(1,500,20),rf_test_score,label="Testing Accuracy")
plt.legend()
plt.plot()

"""#Naive Bayes (Gaussian)

"""

def nb_test(model,x_train,y_train):
  
  print("\n parameters: ")
  #it gives us the default parameters since those are the ones used 
  print( model.get_params(deep=True))
  print("\n log-probability estimates: ")# Finding the log Probablity Estimates
  print(model.predict_log_proba(x_train))
  print("\n probability estimates: ") #Finding the probablity Estimates
  print(model.predict_proba(x_train))

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
Gmodel=GaussianNB()
Gmodel.fit(x_train,y_train.ravel())
train_pred_gau=Gmodel.predict(x_train) #Predicting on training data
test_pred_gau=Gmodel.predict(x_test) #Predicting on Testing data

nb_test(Gmodel,x_train,y_train)
print("accuracy",accuracy_score(y_test, test_pred_gau))
print("Precision:", precision_score(y_test, test_pred_gau, average='macro'))
print("Recall:", recall_score(y_test, test_pred_gau, average='macro'))
print("F1 Score:", f1_score(y_test, test_pred_gau, average='macro'))
print("Cohen Kappa Score",cohen_kappa_score(y_test, test_pred_gau))
print(classification_report(y_test, test_pred_gau)) # Classification Report
cm=confusion_matrix(y_test,test_pred_gau)
print("\n Confusion Matrix: ")
sns.heatmap(cm, cmap="Blues", annot=True,annot_kws={"size": 18}, fmt='g') #Confusion Matrix

"""#Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier
# Create Decision Tree classifer object
clf = DecisionTreeClassifier(criterion="entropy")

# Train Decision Tree Classifer
clf = clf.fit(x_train,y_train)

#Predict the response for test dataset
y_pred = clf.predict(x_test)

# Model Accuracy, how often is the classifier correct?
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

#Performance function : takes y_test and y_pred arrays and prints classification report, confusion matrix
def performance(y_test,y_pred):
  print("accuracy",accuracy_score(y_test, y_pred))
  cm=confusion_matrix(y_test,y_pred)
  print(cm)
  print(classification_report(y_test, y_pred))
  from sklearn.metrics import cohen_kappa_score
  print("Cohen Kappa Score",cohen_kappa_score(y_test, y_pred))
performance(y_test, y_pred)

#Performing GridsearCV on Decision tree to find best parameters
param_grid = {'criterion':['gini','entropy'],
              'random_state':[1,2,3],
              'max_features': ['auto', 'sqrt', 'log2'],
              'max_leaf_nodes':[None, 1,2,5,10,15,20,30],
              'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, None]}

GS_DT=GridSearchCV(RandomForestClassifier(), param_grid)
GS_DT.fit(x_train,y_train.ravel())
print(GS_RF.best_params_)
print(GS_RF.score(x_test,y_test))

!pip install graphviz
!pip install pydotplus
!pip install --upgrade scikit-learn==0.20.3

#For plotting the tree
from sklearn.tree import export_graphviz
from sklearn.externals.six import StringIO  
from IPython.display import Image  
import pydotplus

dot_data = StringIO()
export_graphviz(clf, out_file=dot_data,  
                filled=True, rounded=True,
                special_characters=True,feature_names = df.columns[:-1],class_names=['0','1','2','3'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png('diabetes.png')
Image(graph.create_png())

#Accuracy vs max-depth


rf_train_score=[]
rf_test_score=[]
for i in np.arange(1, 30):
  param_grid = {'criterion':['gini','entropy'],'max_depth': [i],'random_state':[2,3]}
  GS_RF=GridSearchCV(DecisionTreeClassifier(), param_grid,cv=5)
  GS_RF.fit(x_train,y_train.ravel())
  y_train_pred=GS_RF.predict(x_train)
  rf_train_score.append(accuracy_score(y_train,y_train_pred))
  y_pred=GS_RF.predict(x_test)
  rf_test_score.append(accuracy_score(y_test,y_pred))
  # print(rf_train_score,rf_test_score)
 
plt.title("DecisionTree Classifier : Accuracy vs Max Depth")
plt.xlabel("Max Depth")
plt.ylabel("Accuracy")
plt.plot(np.arange(1,30),rf_train_score,label="Training Accuracy")
plt.plot(np.arange(1,30),rf_test_score,label="Testing Accuracy")
plt.legend()
plt.plot()

"""#K-Nearest Neighbour(euclidean_distance (l2))"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
import pickle
error_rate = []
acc = []
ps=[]
rs=[]
fs=[]
for i in range(1,50):
 knn = KNeighborsClassifier(n_neighbors=i) #Power Parameter DEFAULT p=2
 knn.fit(x_train,y_train)
 pred_i = knn.predict(x_test)
 error_rate.append(np.mean(pred_i != y_test))
 acc.append(accuracy_score(y_test, pred_i))#Storing the accuracy score in list
 ps.append(precision_score(y_test, pred_i, average='macro')) #Storing the precision score in list
 rs.append(recall_score(y_test, pred_i, average='macro')) #Storing recall score in list
 fs.append(f1_score(y_test, pred_i, average='macro')) #Storing f1 score in list
 file1 = open("KNN(E)", "wb") #Storing the weights of the model
 pickle.dump(knn, file1) #Model trained on grid search parameters
 file1.close()
 
#ERROR RATE vs K-VALUE
plt.figure(figsize=(10,6))
plt.plot(range(1,50),error_rate,color='blue', linestyle='dashed', marker='o',markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')
print("Minimum error:",min(error_rate),"at K =",error_rate.index(min(error_rate))+1)

#ACCURACY vs K-VALUE
plt.figure(figsize=(10,6))
plt.plot(range(1,50),acc,color = 'blue',linestyle='dashed', marker='o',markerfacecolor='red', markersize=10)
plt.title('accuracy vs. K Value')
plt.xlabel('K')
plt.ylabel('Accuracy')
#We find the model with minimum error and maximum accuracy
print("Maximum accuracy:",max(acc),"at K =",acc.index(max(acc))+1)
print("Maximum precision score:",max(acc),"at K =",ps.index(max(ps))+1)
print("Maximum recall score:",max(acc),"at K =",rs.index(max(rs))+1)
print("Maximum f1 score:",max(acc),"at K =",fs.index(max(fs))+1)


knn = KNeighborsClassifier(n_neighbors=acc.index(max(acc))+1) 
knn.fit(x_train,y_train)
pred = knn.predict(x_test)
file1 = open("KNN(E)", "wb") #Storing the weights of the model
pickle.dump(knn, file1) #Model trained on grid search parameters
file1.close()
print(classification_report(y_test, pred))# Classification Report
sklearn.metrics.plot_confusion_matrix(knn,x_test,y_test,cmap=plt.cm.Blues)    #Confusion Matrix

knn = KNeighborsClassifier(n_neighbors=acc.index(max(acc))+1) 
knn.fit(x_train,y_train)
pred = knn.predict(x_test)
print(classification_report(y_test, pred)) # Classification Report
comf=confusion_matrix(y_test, pred) #Confusion Matrix
sns.heatmap(comf, cmap="Blues", annot=True,annot_kws={"size": 18}, fmt='g')

"""#K-Nearest Neighbour(manhattan_distance (l1))"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
import pickle
error_rate = []
acc = []
ps=[]
rs=[]
fs=[]
for i in range(1,50):
 knn = KNeighborsClassifier(n_neighbors=i,p=1) #Power Parameter: manhattan_distance (l1)
 knn.fit(x_train,y_train)
 pred_i = knn.predict(x_test)
 error_rate.append(np.mean(pred_i != y_test))
 acc.append(accuracy_score(y_test, pred_i))#Storing the accuracy score in list
 ps.append(precision_score(y_test, pred_i, average='macro'))#Storing the precision score in list
 rs.append(recall_score(y_test, pred_i, average='macro'))#Storing recall score in list
 fs.append(f1_score(y_test, pred_i, average='macro')) #Storing f1 score in list
 file1 = open("KNN(M)", "wb") #Storing the weights of the model 
 pickle.dump(knn, file1) 
 file1.close()
 

 
#ERROR RATE vs K-VALUE
plt.figure(figsize=(10,6))
plt.plot(range(1,50),error_rate,color='blue', linestyle='dashed', marker='o',markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value(M)')
plt.xlabel('K')
plt.ylabel('Error Rate')
print("Minimum error:",min(error_rate),"at K =",error_rate.index(min(error_rate))+1)

#ACCURACY vs K-VALUE
plt.figure(figsize=(10,6))
plt.plot(range(1,50),acc,color = 'blue',linestyle='dashed', marker='o',markerfacecolor='red', markersize=10)
plt.title('accuracy vs. K Value(M)')
plt.xlabel('K')
plt.ylabel('Accuracy')
#We find the model with minimum error and maximum accuracy
print("Maximum accuracy:",max(acc),"at K =",acc.index(max(acc))+1)
print("Maximum precision score:",max(acc),"at K =",ps.index(max(ps))+1)
print("Maximum recall score:",max(acc),"at K =",rs.index(max(rs))+1)
print("Maximum f1 score:",max(acc),"at K =",fs.index(max(fs))+1)

knn = KNeighborsClassifier(n_neighbors=acc.index(max(acc))+1,p=1) #We find the model with minimum error and maximum accuracy
knn.fit(x_train,y_train)
pred = knn.predict(x_test)
import pickle
file1 = open("KNN(M)", "wb") #Storing the weights of the model
pickle.dump(knn, file1) 
file1.close()
print(classification_report(y_test, pred))# Classification Report
comf=confusion_matrix(y_test, pred)#Confusion Matrix
print(sns.heatmap(comf, cmap="Blues", annot=True,annot_kws={"size": 18}, fmt='g'))

k=[]
for i in range(1,50):
  k.append(i)
#We use gridsearchCV to find the best possible paramters
param_grid={'n_neighbors':[1,2,3,4,5,6,7,8,9,10],'weights':['uniform','distance'],'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],'leaf_size':[30],'p':[1,2]}
knn = GridSearchCV(KNeighborsClassifier(), param_grid)
#knn = KNeighborsClassifier(n_neighbors=acc.index(max(acc))+1) 
knn.fit(x_train, y_train.ravel())#Model trained on grid search parameters
print(knn.best_params_)
pred = knn.predict(x_test)
print(classification_report(y_test, pred))# Classification Report
sklearn.metrics.plot_confusion_matrix(knn,x_test,y_test,cmap=plt.cm.Blues)#Confusion Matrix

"""# SVM

"""

#importing necessary libraries
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
#Performance function : takes y_test and y_pred arrays and prints classification report, confusion matrix
def performance(y_test,y_pred):
  print("accuracy",accuracy_score(y_test, y_pred))
  cm=confusion_matrix(y_test,y_pred)
  print(cm)
  print(classification_report(y_test, y_pred))
  from sklearn.metrics import cohen_kappa_score
  print("Cohen Kappa Score",cohen_kappa_score(y_test, y_pred))

#Linear kernel SVM
from sklearn import svm
#Making pipeline
SVM_object = make_pipeline(svm.SVC(kernel='poly'))
#Fitting training data
SVM_object.fit(x_train,y_train)
y_pred=SVM_object.predict(x_test)
#Calling performance function
performance(y_test, y_pred)

#Plotting using seaborn
import seaborn as sns
comf=confusion_matrix(y_test, y_pred)
sns.heatmap(comf, cmap="Blues", annot=True,annot_kws={"size": 18}, fmt='g')

#Best kernel Performance using Grid Search
param_grid = {'kernel':['linear','poly','sigmoid','rbf'],'gamma':['scale','auto'],'random_state':[1,2,3]}
GS_SVM=GridSearchCV(svm.SVC(), param_grid,cv=5)
GS_SVM.fit(x_train,y_train)
GS_SVM.best_params_

GS_SVM.score(x_test,y_test)

#Error vs Kernel plot
dt_train_score=[]
dt_test_score=[]
for i in ['linear','poly','sigmoid','rbf']:
  param_grid = {'kernel':[i],'gamma':['scale','auto'],'random_state':[1,2,3]}
  GS_SVM=GridSearchCV(svm.SVC(), param_grid,cv=5)
  GS_SVM.fit(x_train,y_train)
  y_train_pred=GS_SVM.predict(x_train)
  y_pred=GS_SVM.predict(x_test)
  dt_train_score.append(accuracy_score(y_train,y_train_pred))
  dt_test_score.append(accuracy_score(y_test,y_pred))

#Accuracy vs kernel
plt.title("SVM: Accuracy vs kernel")
plt.xlabel("Kernel")
plt.ylabel("Accuracy")
plt.plot(['linear','poly','sigmoid','rbf'],dt_train_score,label="Training Accuracy")
plt.plot(['linear','poly','sigmoid','rbf'],dt_test_score,label="Testing Accuracy")
plt.legend()
plt.plot()

#Importing libraries for tsne plots
from sklearn.manifold import TSNE
from sklearn.decomposition import TruncatedSVD
tsne = TSNE(n_components=2, verbose=1, random_state=123)
z = tsne.fit_transform(x_train)

df1= pd.DataFrame()
df1["y"] = y_train
df1["comp-1"] = z[:,0]
df1["comp-2"] = z[:,1]

#some convert lists of lists to 2 dataframes (df_train_neg, df_train_pos) depending on the label - 

#plot the negative points and positive points
sns.scatterplot(x="comp-1", y="comp-2", hue=df1.y.tolist(),
                palette=sns.color_palette("hls", 4),
                data=df1).set(title="Stroke T-SNE projection")

#PCA transform. n_components = 2
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pca.fit(x_train)
x_pca = pca.transform(x_train)

x_pca

#Code source: sklearn
#For plotting decision boundary and decision space of different SVM kernels
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets


def make_meshgrid(x, y, h=0.02):
    """Create a mesh of points to plot in

    Parameters
    ----------
    x: data to base x-axis meshgrid on
    y: data to base y-axis meshgrid on
    h: stepsize for meshgrid, optional

    Returns
    -------
    xx, yy : ndarray
    """
    x_min, x_max = x.min() - 1, x.max() + 1
    y_min, y_max = y.min() - 1, y.max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    return xx, yy


def plot_contours(ax, clf, xx, yy, **params):
    """Plot the decision boundaries for a classifier.

    Parameters
    ----------
    ax: matplotlib axes object
    clf: a classifier
    xx: meshgrid ndarray
    yy: meshgrid ndarray
    params: dictionary of params to pass to contourf, optional
    """
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    out = ax.contourf(xx, yy, Z, **params)
    return out


# import some data to play with
iris = datasets.load_iris()
# Take the first two features. We could avoid this by using a two-dim dataset
X = x_pca
y = y_train

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
models = (
    svm.SVC(kernel="linear", C=C),
    svm.SVC(kernel="sigmoid", C=C),
    svm.SVC(kernel="rbf", gamma=0.7, C=C),
    svm.SVC(kernel="poly", degree=3, gamma="auto", C=C),
)
models = (clf.fit(X, y) for clf in models)

# title for the plots
titles = (
    "SVC with linear kernel",
    "SVC with sigmoid kernel",
    "SVC with RBF kernel",
    "SVC with polynomial (degree 3) kernel",
)

# Set-up 2x2 grid for plotting.
fig, sub = plt.subplots(2, 2, figsize=(10,8))
plt.subplots_adjust(wspace=0.8, hspace=0.8)

X0, X1 = X[:, 0], X[:, 1]
xx, yy = make_meshgrid(X0, X1)


for clf, title, ax in zip(models, titles, sub.flatten()):
 
    plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)
    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors="k")
    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xlabel("Component1")
    ax.set_ylabel("Component2")
    ax.set_xticks(())
    ax.set_yticks(())
    ax.set_title(title)

plt.show()

"""# Feature importance

Found above using RF classifier

NIHSS_Score    0.232721
tos            0.126347
bmi            0.126228
mRS            0.116398
glucose        0.097961
systolic       0.080692
paralysis      0.066406
cholestrol     0.059491
diastolic      0.044202
age            0.023455
smoking        0.021836
Male           0.002139
Female         0.002124
"""

x = ["NIHSS_Score", "tos", "bmi", "mRS", "glucose ", "systolic ", "paralysis", "cholestrol", "diastolic", "age", "smoking", "Male" ,"Female"]
y = [0.232721, 0.126347, 0.126228, 0.116398, 0.097961, 0.080692, 0.066406, 0.059491 , 0.044202, 0.023455,0.021836,0.002139,0.002124]
plt.figure(figsize = (12,6))
plt.bar(x,y)
plt.xlabel("Feature")
plt.ylabel("Importance")
plt.title("Feature importance")